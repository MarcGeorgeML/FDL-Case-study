{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "374279e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.set_float32_matmul_precision('high')\n",
    "torch._dynamo.config.suppress_errors = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9f6e0724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from collections import Counter\n",
    "from torch.optim.lr_scheduler import OneCycleLR\n",
    "import torch_optimizer as optim_advanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f46dd210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:01<00:00, 7009.51it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Setup\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "OUT_DIR = \"/content/flight_optuna_best\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Synthetic balanced dataset\n",
    "N_SEQ, TIMESTEPS, FEATURES = 8000, 60, 7\n",
    "def generate_sequence(is_incident, timesteps=60):\n",
    "    t = np.linspace(0,1,timesteps)\n",
    "    alt = 10000 + 200*np.sin(2*np.pi*t) + np.random.normal(0,8,timesteps)\n",
    "    spd = 250 + 8*np.cos(2*np.pi*t) + np.random.normal(0,3,timesteps)\n",
    "    pitch, roll, yaw = np.random.normal(0,0.8,timesteps), np.random.normal(0,0.8,timesteps), np.random.normal(0,0.5,timesteps)\n",
    "    eng, thrust = 200+np.random.normal(0,2,timesteps), 70+np.random.normal(0,2,timesteps)\n",
    "    seq = np.vstack([alt,spd,pitch,roll,yaw,eng,thrust]).T\n",
    "    weather, phase, maint = random.choice([\"clear\",\"rain\",\"storm\",\"fog\",\"turbulence\"]), random.choice([\"cruise\",\"takeoff\",\"landing\",\"climb\",\"descent\"]), np.random.randint(0,3)\n",
    "    if is_incident:\n",
    "        pat = random.choice([\"rapid_descent\",\"stall\",\"engine_fail\"])\n",
    "        if pat==\"rapid_descent\": seq[:,0]-=np.linspace(0,np.random.randint(1000,1500),timesteps)\n",
    "        if pat==\"stall\": seq[:,1]-=np.linspace(0,np.random.randint(60,120),timesteps); seq[:,2]+=np.linspace(0,np.random.randint(4,8),timesteps)\n",
    "        if pat==\"engine_fail\": seq[:,5]+=np.linspace(0,np.random.randint(100,150),timesteps); seq[:,6]-=np.linspace(0,np.random.randint(30,50),timesteps)\n",
    "        maint+=np.random.randint(1,2); weather=random.choice([\"storm\",\"turbulence\",\"fog\"])\n",
    "    return seq, weather, phase, maint, int(is_incident)\n",
    "\n",
    "seqs, metas, labels = [], [], []\n",
    "for i in tqdm(range(N_SEQ)):\n",
    "    seq,w,p,m,y = generate_sequence(i>=N_SEQ//2)\n",
    "    seqs.append(seq); metas.append((w,p,m)); labels.append(y)\n",
    "seqs=np.array(seqs); metas=pd.DataFrame(metas,columns=[\"weather\",\"phase\",\"maint\"]); labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "56b70a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c12bb3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, scheduler, device, accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0.0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    for i, (sequences, metadata, labels) in enumerate(dataloader):\n",
    "        sequences = sequences.to(device)\n",
    "        metadata = metadata.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(sequences, metadata)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        # Normalize loss for gradient accumulation\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Update weights every `accumulation_steps` batches\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "        # Scale loss back for logging\n",
    "        total_loss += loss.item() * accumulation_steps\n",
    "\n",
    "        # Compute accuracy\n",
    "        preds = (torch.sigmoid(outputs) > 0.65).float()\n",
    "        correct += (preds == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    # Handle remaining gradients if dataset size not divisible by accumulation_steps\n",
    "    if total % accumulation_steps != 0:\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "        optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "\n",
    "    return avg_loss, accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1b68c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for sequences, metadata, labels in dataloader:\n",
    "            sequences = sequences.to(device)\n",
    "            metadata = metadata.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(sequences, metadata)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            predictions = (probs > 0.65).float()\n",
    "            \n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "431ceccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: 8000 samples, 60 timesteps, 7 features\n"
     ]
    }
   ],
   "source": [
    "N, T, F = seqs.shape\n",
    "print(f\"Data shape: {N} samples, {T} timesteps, {F} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5d27a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess metadata\n",
    "cat_cols = [\"weather\", \"phase\"]\n",
    "num_cols = [\"maint\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "252fdaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "fc919b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_processed = preprocessor.fit_transform(metas)\n",
    "\n",
    "if hasattr(X_meta_processed, 'toarray'):\n",
    "    X_meta_processed = X_meta_processed.toarray()\n",
    "    \n",
    "num_meta_features = X_meta_processed.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "99f14f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 6400 samples\n",
      "Validation: 1600 samples\n"
     ]
    }
   ],
   "source": [
    "# Train/validation split\n",
    "X_seq_train, X_seq_val, X_meta_train, X_meta_val, y_train, y_val = train_test_split(\n",
    "    seqs, X_meta_processed, labels, \n",
    "    test_size=0.2, \n",
    "    stratify=labels, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(X_seq_train)} samples\")\n",
    "print(f\"Validation: {len(X_seq_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "359306be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import AviationTransformer\n",
    "\n",
    "model = AviationTransformer(\n",
    "    num_features=seqs.shape[2],\n",
    "    num_meta_features=X_meta_train.shape[1],\n",
    "    d_model=256,\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.05,\n",
    "    layerdrop=0.05,\n",
    "    max_seq_len=seqs.shape[1]\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1aa00fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_batch_size(model, device, max_batch=512):\n",
    "    \"\"\"\n",
    "    Binary search for the largest batch size that fits in memory,\n",
    "    using the model's actual metadata dimension.\n",
    "    \"\"\"\n",
    "    # Extract metadata dimension from model\n",
    "    # meta_fc is: Linear(in_features=num_meta_features, out_features=64)\n",
    "    num_meta = model.meta_fc[0].in_features\n",
    "\n",
    "    batch_size = 32\n",
    "    while batch_size <= max_batch:\n",
    "        try:\n",
    "            # Create dummy inputs with the correct shapes\n",
    "            dummy_seq  = torch.randn(batch_size, 60, 7, device=device)\n",
    "            dummy_meta = torch.randn(batch_size, num_meta, device=device)\n",
    "\n",
    "            outputs = model(dummy_seq, dummy_meta)\n",
    "            loss = outputs.sum()\n",
    "            loss.backward()\n",
    "            return batch_size  # success\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                # Too large: try half\n",
    "                torch.cuda.empty_cache()\n",
    "                batch_size //= 2\n",
    "                if batch_size == 0:\n",
    "                    raise RuntimeError(\"Not enough GPU memory even for batch_size=1\")\n",
    "            else:\n",
    "                # Propagate other errors\n",
    "                raise e\n",
    "\n",
    "    return batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3df78cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal batch size: 32\n"
     ]
    }
   ],
   "source": [
    "optimal_batch = find_optimal_batch_size(model, device)\n",
    "print(f\"Optimal batch size: {optimal_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c65dc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AviationDataset\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = AviationDataset(X_seq_train, X_meta_train, y_train)\n",
    "val_dataset = AviationDataset(X_seq_val, X_meta_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=optimal_batch, shuffle=True, \n",
    "                          num_workers=4, pin_memory=True, prefetch_factor=2, \n",
    "                          persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=optimal_batch, shuffle=False, \n",
    "                        num_workers=2, pin_memory=True, prefetch_factor=2, \n",
    "                        persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cc74bd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "AviationTransformer(\n",
      "  (input_projection): Linear(in_features=7, out_features=256, bias=True)\n",
      "  (pos_encoder): PositionalEncoding(\n",
      "    (dropout): Dropout(p=0.05, inplace=False)\n",
      "  )\n",
      "  (transformer_encoder): TransformerEncoder(\n",
      "    (layers): ModuleList(\n",
      "      (0-3): 4 x TransformerBlockWithLayerDrop(\n",
      "        (self_attn): MultiheadAttention(\n",
      "          (out_proj): NonDynamicallyQuantizableLinear(in_features=256, out_features=256, bias=True)\n",
      "        )\n",
      "        (linear1): Linear(in_features=256, out_features=512, bias=True)\n",
      "        (dropout): Dropout(p=0.05, inplace=False)\n",
      "        (linear2): Linear(in_features=512, out_features=256, bias=True)\n",
      "        (norm1): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (norm2): LayerNorm((256,), eps=1e-05, elementwise_affine=True)\n",
      "        (dropout1): Dropout(p=0.05, inplace=False)\n",
      "        (dropout2): Dropout(p=0.05, inplace=False)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (meta_fc): Sequential(\n",
      "    (0): Linear(in_features=11, out_features=64, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.05, inplace=False)\n",
      "  )\n",
      "  (classifier): Sequential(\n",
      "    (0): Linear(in_features=320, out_features=256, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Dropout(p=0.05, inplace=False)\n",
      "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "    (4): ReLU()\n",
      "    (5): Dropout(p=0.05, inplace=False)\n",
      "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 2,226,433\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33c43023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "from loss import FocalLoss\n",
    "class_counts = Counter(y_train)\n",
    "pos_weight = torch.tensor([class_counts[0] / class_counts[1]]).to(device)\n",
    "criterion = FocalLoss(alpha=[0.5,0.5], gamma=2.0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "866a3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bb65e5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marc\\anaconda3\\envs\\casenv\\Lib\\site-packages\\pytorch_ranger\\ranger.py:175: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha = 1) (Triggered internally at C:\\cb\\pytorch_1000000000000\\work\\torch\\csrc\\utils\\python_arg_parser.cpp:1642.)\n",
      "  exp_avg.mul_(beta1).add_(1 - beta1, grad)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "  Train Loss: 0.0865, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0863, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 2/100\n",
      "  Train Loss: 0.0862, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0861, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 3/100\n",
      "  Train Loss: 0.0860, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0858, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 4/100\n",
      "  Train Loss: 0.0858, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0854, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 5/100\n",
      "  Train Loss: 0.0855, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0852, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 6/100\n",
      "  Train Loss: 0.0852, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0848, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 7/100\n",
      "  Train Loss: 0.0849, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0841, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 8/100\n",
      "  Train Loss: 0.0842, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0838, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 9/100\n",
      "  Train Loss: 0.0838, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0834, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 10/100\n",
      "  Train Loss: 0.0834, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0830, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 11/100\n",
      "  Train Loss: 0.0829, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0826, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 12/100\n",
      "  Train Loss: 0.0824, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0821, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 13/100\n",
      "  Train Loss: 0.0819, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0815, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 14/100\n",
      "  Train Loss: 0.0813, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0808, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 15/100\n",
      "  Train Loss: 0.0806, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0802, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 16/100\n",
      "  Train Loss: 0.0798, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0793, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 17/100\n",
      "  Train Loss: 0.0788, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0787, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 18/100\n",
      "  Train Loss: 0.0778, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0771, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 19/100\n",
      "  Train Loss: 0.0763, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0754, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 20/100\n",
      "  Train Loss: 0.0747, Train Acc: 0.5000\n",
      "  Val   Loss: 0.0735, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 21/100\n",
      "  Train Loss: 0.0727, Train Acc: 0.5002\n",
      "  Val   Loss: 0.0716, Val   Acc: 0.5000\n",
      "  → Model saved!\n",
      "Epoch 22/100\n",
      "  Train Loss: 0.0709, Train Acc: 0.5042\n",
      "  Val   Loss: 0.0698, Val   Acc: 0.5031\n",
      "  → Model saved!\n",
      "Epoch 23/100\n",
      "  Train Loss: 0.0688, Train Acc: 0.5284\n",
      "  Val   Loss: 0.0681, Val   Acc: 0.5031\n",
      "  → Model saved!\n",
      "Epoch 24/100\n",
      "  Train Loss: 0.0672, Train Acc: 0.5731\n",
      "  Val   Loss: 0.0664, Val   Acc: 0.5900\n",
      "  → Model saved!\n",
      "Epoch 25/100\n",
      "  Train Loss: 0.0656, Train Acc: 0.6173\n",
      "  Val   Loss: 0.0650, Val   Acc: 0.6394\n",
      "  → Model saved!\n",
      "Epoch 26/100\n",
      "  Train Loss: 0.0643, Train Acc: 0.6442\n",
      "  Val   Loss: 0.0658, Val   Acc: 0.6156\n",
      "Epoch 27/100\n",
      "  Train Loss: 0.0631, Train Acc: 0.6592\n",
      "  Val   Loss: 0.0628, Val   Acc: 0.6650\n",
      "  → Model saved!\n",
      "Epoch 28/100\n",
      "  Train Loss: 0.0624, Train Acc: 0.6645\n",
      "  Val   Loss: 0.0622, Val   Acc: 0.6650\n",
      "  → Model saved!\n",
      "Epoch 29/100\n",
      "  Train Loss: 0.0612, Train Acc: 0.6725\n",
      "  Val   Loss: 0.0611, Val   Acc: 0.6650\n",
      "  → Model saved!\n",
      "Epoch 30/100\n",
      "  Train Loss: 0.0605, Train Acc: 0.6736\n",
      "  Val   Loss: 0.0598, Val   Acc: 0.6650\n",
      "  → Model saved!\n",
      "Epoch 31/100\n",
      "  Train Loss: 0.0604, Train Acc: 0.6750\n",
      "  Val   Loss: 0.0592, Val   Acc: 0.6687\n",
      "  → Model saved!\n",
      "Epoch 32/100\n",
      "  Train Loss: 0.0592, Train Acc: 0.6786\n",
      "  Val   Loss: 0.0582, Val   Acc: 0.6650\n",
      "  → Model saved!\n",
      "Epoch 33/100\n",
      "  Train Loss: 0.0584, Train Acc: 0.6794\n",
      "  Val   Loss: 0.0570, Val   Acc: 0.6756\n",
      "  → Model saved!\n",
      "Epoch 34/100\n",
      "  Train Loss: 0.0572, Train Acc: 0.6823\n",
      "  Val   Loss: 0.0589, Val   Acc: 0.6650\n",
      "Epoch 35/100\n",
      "  Train Loss: 0.0580, Train Acc: 0.6828\n",
      "  Val   Loss: 0.0562, Val   Acc: 0.6806\n",
      "  → Model saved!\n",
      "Epoch 36/100\n",
      "  Train Loss: 0.0578, Train Acc: 0.6834\n",
      "  Val   Loss: 0.0568, Val   Acc: 0.6963\n",
      "Epoch 37/100\n",
      "  Train Loss: 0.0559, Train Acc: 0.6836\n",
      "  Val   Loss: 0.0553, Val   Acc: 0.6756\n",
      "  → Model saved!\n",
      "Epoch 38/100\n",
      "  Train Loss: 0.0552, Train Acc: 0.6925\n",
      "  Val   Loss: 0.0555, Val   Acc: 0.6650\n",
      "Epoch 39/100\n",
      "  Train Loss: 0.0544, Train Acc: 0.6939\n",
      "  Val   Loss: 0.0524, Val   Acc: 0.6950\n",
      "  → Model saved!\n",
      "Epoch 40/100\n",
      "  Train Loss: 0.0558, Train Acc: 0.6906\n",
      "  Val   Loss: 0.0539, Val   Acc: 0.6687\n",
      "Epoch 41/100\n",
      "  Train Loss: 0.0533, Train Acc: 0.6852\n",
      "  Val   Loss: 0.0558, Val   Acc: 0.7225\n",
      "Epoch 42/100\n",
      "  Train Loss: 0.0525, Train Acc: 0.6998\n",
      "  Val   Loss: 0.0507, Val   Acc: 0.7019\n",
      "  → Model saved!\n",
      "Epoch 43/100\n",
      "  Train Loss: 0.0535, Train Acc: 0.6930\n",
      "  Val   Loss: 0.0519, Val   Acc: 0.6650\n",
      "Epoch 44/100\n",
      "  Train Loss: 0.0526, Train Acc: 0.6955\n",
      "  Val   Loss: 0.0558, Val   Acc: 0.7238\n",
      "Epoch 45/100\n",
      "  Train Loss: 0.0525, Train Acc: 0.7017\n",
      "  Val   Loss: 0.0502, Val   Acc: 0.6706\n",
      "  → Model saved!\n",
      "Epoch 46/100\n",
      "  Train Loss: 0.0525, Train Acc: 0.6923\n",
      "  Val   Loss: 0.0532, Val   Acc: 0.6650\n",
      "Epoch 47/100\n",
      "  Train Loss: 0.0511, Train Acc: 0.6863\n",
      "  Val   Loss: 0.0486, Val   Acc: 0.6725\n",
      "  → Model saved!\n",
      "Epoch 48/100\n",
      "  Train Loss: 0.0504, Train Acc: 0.6995\n",
      "  Val   Loss: 0.0480, Val   Acc: 0.7219\n",
      "  → Model saved!\n",
      "Epoch 49/100\n",
      "  Train Loss: 0.0499, Train Acc: 0.7013\n",
      "  Val   Loss: 0.0555, Val   Acc: 0.7250\n",
      "Epoch 50/100\n",
      "  Train Loss: 0.0498, Train Acc: 0.6952\n",
      "  Val   Loss: 0.0566, Val   Acc: 0.7531\n",
      "Epoch 51/100\n",
      "  Train Loss: 0.0502, Train Acc: 0.6970\n",
      "  Val   Loss: 0.0470, Val   Acc: 0.7100\n",
      "  → Model saved!\n",
      "Epoch 52/100\n",
      "  Train Loss: 0.0485, Train Acc: 0.6997\n",
      "  Val   Loss: 0.0454, Val   Acc: 0.7144\n",
      "  → Model saved!\n",
      "Epoch 53/100\n",
      "  Train Loss: 0.0490, Train Acc: 0.6958\n",
      "  Val   Loss: 0.0512, Val   Acc: 0.6650\n",
      "Epoch 54/100\n",
      "  Train Loss: 0.0482, Train Acc: 0.6948\n",
      "  Val   Loss: 0.0520, Val   Acc: 0.6650\n",
      "Epoch 55/100\n",
      "  Train Loss: 0.0479, Train Acc: 0.6939\n",
      "  Val   Loss: 0.0433, Val   Acc: 0.7094\n",
      "  → Model saved!\n",
      "Epoch 56/100\n",
      "  Train Loss: 0.0474, Train Acc: 0.7056\n",
      "  Val   Loss: 0.0496, Val   Acc: 0.7375\n",
      "Epoch 57/100\n",
      "  Train Loss: 0.0464, Train Acc: 0.7053\n",
      "  Val   Loss: 0.0533, Val   Acc: 0.7700\n",
      "Epoch 58/100\n",
      "  Train Loss: 0.0472, Train Acc: 0.6919\n",
      "  Val   Loss: 0.0441, Val   Acc: 0.6669\n",
      "Epoch 59/100\n",
      "  Train Loss: 0.0468, Train Acc: 0.6894\n",
      "  Val   Loss: 0.0425, Val   Acc: 0.6725\n",
      "  → Model saved!\n",
      "Epoch 60/100\n",
      "  Train Loss: 0.0449, Train Acc: 0.7008\n",
      "  Val   Loss: 0.0400, Val   Acc: 0.7369\n",
      "  → Model saved!\n",
      "Epoch 61/100\n",
      "  Train Loss: 0.0466, Train Acc: 0.6980\n",
      "  Val   Loss: 0.0423, Val   Acc: 0.6763\n",
      "Epoch 62/100\n",
      "  Train Loss: 0.0455, Train Acc: 0.7019\n",
      "  Val   Loss: 0.0441, Val   Acc: 0.6650\n",
      "Epoch 63/100\n",
      "  Train Loss: 0.0451, Train Acc: 0.6991\n",
      "  Val   Loss: 0.0420, Val   Acc: 0.6656\n",
      "Epoch 64/100\n",
      "  Train Loss: 0.0444, Train Acc: 0.7056\n",
      "  Val   Loss: 0.0488, Val   Acc: 0.6650\n",
      "Epoch 65/100\n",
      "  Train Loss: 0.0441, Train Acc: 0.7036\n",
      "  Val   Loss: 0.0394, Val   Acc: 0.7425\n",
      "  → Model saved!\n",
      "Epoch 66/100\n",
      "  Train Loss: 0.0475, Train Acc: 0.6933\n",
      "  Val   Loss: 0.0422, Val   Acc: 0.6650\n",
      "Epoch 67/100\n",
      "  Train Loss: 0.0458, Train Acc: 0.6952\n",
      "  Val   Loss: 0.0460, Val   Acc: 0.7925\n",
      "Epoch 68/100\n",
      "  Train Loss: 0.0446, Train Acc: 0.7034\n",
      "  Val   Loss: 0.0430, Val   Acc: 0.6650\n",
      "Epoch 69/100\n",
      "  Train Loss: 0.0428, Train Acc: 0.7131\n",
      "  Val   Loss: 0.0437, Val   Acc: 0.6650\n",
      "Epoch 70/100\n",
      "  Train Loss: 0.0448, Train Acc: 0.7013\n",
      "  Val   Loss: 0.0440, Val   Acc: 0.7863\n",
      "Epoch 71/100\n",
      "  Train Loss: 0.0445, Train Acc: 0.6998\n",
      "  Val   Loss: 0.0432, Val   Acc: 0.6650\n",
      "Epoch 72/100\n",
      "  Train Loss: 0.0449, Train Acc: 0.7014\n",
      "  Val   Loss: 0.0428, Val   Acc: 0.6650\n",
      "Epoch 73/100\n",
      "  Train Loss: 0.0428, Train Acc: 0.7163\n",
      "  Val   Loss: 0.0452, Val   Acc: 0.6650\n",
      "Epoch 74/100\n",
      "  Train Loss: 0.0440, Train Acc: 0.6911\n",
      "  Val   Loss: 0.0441, Val   Acc: 0.7606\n",
      "Epoch 75/100\n",
      "  Train Loss: 0.0440, Train Acc: 0.7063\n",
      "  Val   Loss: 0.0457, Val   Acc: 0.6650\n",
      "Epoch 76/100\n",
      "  Train Loss: 0.0447, Train Acc: 0.6789\n",
      "  Val   Loss: 0.0504, Val   Acc: 0.6650\n",
      "Epoch 77/100\n",
      "  Train Loss: 0.0434, Train Acc: 0.7014\n",
      "  Val   Loss: 0.0385, Val   Acc: 0.7350\n",
      "  → Model saved!\n",
      "Epoch 78/100\n",
      "  Train Loss: 0.0423, Train Acc: 0.7262\n",
      "  Val   Loss: 0.0425, Val   Acc: 0.6656\n",
      "Epoch 79/100\n",
      "  Train Loss: 0.0421, Train Acc: 0.7255\n",
      "  Val   Loss: 0.0410, Val   Acc: 0.6894\n",
      "Epoch 80/100\n",
      "  Train Loss: 0.0425, Train Acc: 0.7236\n",
      "  Val   Loss: 0.0415, Val   Acc: 0.6944\n",
      "Epoch 81/100\n",
      "  Train Loss: 0.0430, Train Acc: 0.7269\n",
      "  Val   Loss: 0.0453, Val   Acc: 0.6650\n",
      "Epoch 82/100\n",
      "  Train Loss: 0.0439, Train Acc: 0.7041\n",
      "  Val   Loss: 0.0445, Val   Acc: 0.6650\n",
      "Epoch 83/100\n",
      "  Train Loss: 0.0446, Train Acc: 0.6716\n",
      "  Val   Loss: 0.0436, Val   Acc: 0.6650\n",
      "Epoch 84/100\n",
      "  Train Loss: 0.0432, Train Acc: 0.6972\n",
      "  Val   Loss: 0.0428, Val   Acc: 0.6650\n",
      "Epoch 85/100\n",
      "  Train Loss: 0.0417, Train Acc: 0.7206\n",
      "  Val   Loss: 0.0384, Val   Acc: 0.7444\n",
      "  → Model saved!\n",
      "Epoch 86/100\n",
      "  Train Loss: 0.0423, Train Acc: 0.7317\n",
      "  Val   Loss: 0.0386, Val   Acc: 0.7281\n",
      "Epoch 87/100\n",
      "  Train Loss: 0.0400, Train Acc: 0.7492\n",
      "  Val   Loss: 0.0449, Val   Acc: 0.6650\n",
      "Epoch 88/100\n",
      "  Train Loss: 0.0442, Train Acc: 0.7180\n",
      "  Val   Loss: 0.0451, Val   Acc: 0.6650\n",
      "Epoch 89/100\n",
      "  Train Loss: 0.0447, Train Acc: 0.6709\n",
      "  Val   Loss: 0.0444, Val   Acc: 0.6650\n",
      "Epoch 90/100\n",
      "  Train Loss: 0.0448, Train Acc: 0.6716\n",
      "  Val   Loss: 0.0445, Val   Acc: 0.6650\n",
      "Epoch 91/100\n",
      "  Train Loss: 0.0447, Train Acc: 0.6722\n",
      "  Val   Loss: 0.0451, Val   Acc: 0.6650\n",
      "Epoch 92/100\n",
      "  Train Loss: 0.0445, Train Acc: 0.6714\n",
      "  Val   Loss: 0.0444, Val   Acc: 0.6650\n",
      "Epoch 93/100\n",
      "  Train Loss: 0.0442, Train Acc: 0.6717\n",
      "  Val   Loss: 0.0441, Val   Acc: 0.6650\n",
      "Epoch 94/100\n",
      "  Train Loss: 0.0442, Train Acc: 0.6722\n",
      "  Val   Loss: 0.0438, Val   Acc: 0.6650\n",
      "Epoch 95/100\n",
      "  Train Loss: 0.0445, Train Acc: 0.6781\n",
      "  Val   Loss: 0.0438, Val   Acc: 0.6650\n",
      "Epoch 96/100\n",
      "  Train Loss: 0.0438, Train Acc: 0.6845\n",
      "  Val   Loss: 0.0442, Val   Acc: 0.6650\n",
      "Epoch 97/100\n",
      "  Train Loss: 0.0434, Train Acc: 0.6923\n",
      "  Val   Loss: 0.0448, Val   Acc: 0.6650\n",
      "Epoch 98/100\n",
      "  Train Loss: 0.0450, Train Acc: 0.6895\n",
      "  Val   Loss: 0.0437, Val   Acc: 0.6650\n",
      "Epoch 99/100\n",
      "  Train Loss: 0.0431, Train Acc: 0.6830\n",
      "  Val   Loss: 0.0497, Val   Acc: 0.6650\n",
      "Epoch 100/100\n",
      "  Train Loss: 0.0436, Train Acc: 0.6944\n",
      "  Val   Loss: 0.0442, Val   Acc: 0.6650\n",
      "\n",
      "Early stopping after 100 epochs\n"
     ]
    }
   ],
   "source": [
    "# Ensure directory exists\n",
    "os.makedirs('models', exist_ok=True)\n",
    "\n",
    "# Training loop\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "# Optimizer and OneCycleLR\n",
    "optimizer = optim_advanced.Ranger(model.parameters(), lr=3e-4, weight_decay=5e-5)\n",
    "scheduler = OneCycleLR(\n",
    "    optimizer,\n",
    "    max_lr=5e-4,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=len(train_loader),\n",
    "    pct_start=0.3,\n",
    "    anneal_strategy='cos',\n",
    "    div_factor=25.0,\n",
    "    final_div_factor=1e4\n",
    ")\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    # Train for one epoch (scheduling inside)\n",
    "    train_loss, train_acc = train_epoch(\n",
    "        model,\n",
    "        train_loader,\n",
    "        criterion,\n",
    "        optimizer,\n",
    "        scheduler,\n",
    "        device,\n",
    "        accumulation_steps=4\n",
    "    )\n",
    "\n",
    "    # Validate\n",
    "    val_loss, val_acc = validate(model, val_loader, criterion, device)\n",
    "\n",
    "    # Record metrics\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "\n",
    "    # Log\n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val   Loss: {val_loss:.4f}, Val   Acc: {val_acc:.4f}\")\n",
    "\n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'models/best_aviation_transformer.pt')\n",
    "        print(\"  → Model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping after {epoch+1} epochs\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "db770275",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Marc\\AppData\\Local\\Temp\\ipykernel_2176\\154976086.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  model.load_state_dict(torch.load('models/best_aviation_transformer.pt'))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('models/best_aviation_transformer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "ea6a348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "def full_validate(model, dataloader, criterion, device):\n",
    "    # Run your existing validate to get loss and accuracy\n",
    "    val_loss, val_acc = validate(model, dataloader, criterion, device)\n",
    "\n",
    "    # Now collect all labels, predictions, and probabilities\n",
    "    model.eval()\n",
    "    all_labels, all_preds, all_probs = [], [], []\n",
    "    with torch.inference_mode():\n",
    "        for seqs, metas, labels in dataloader:\n",
    "            seqs, metas = seqs.to(device), metas.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            outputs = model(seqs, metas)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_probs.extend(probs.cpu().numpy().flatten())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "    y_prob = np.array(all_probs)\n",
    "\n",
    "    # Compute standard metrics\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    prec  = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec   = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1    = f1_score(y_true, y_pred, zero_division=0)\n",
    "    roc   = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else None\n",
    "    prauc = average_precision_score(y_true, y_prob)\n",
    "\n",
    "    # Confusion matrix and classification report\n",
    "    cm     = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Accuracy:        {acc:.4f}\")\n",
    "    print(f\"Precision:       {prec:.4f}\")\n",
    "    print(f\"Recall:          {rec:.4f}\")\n",
    "    print(f\"F1 Score:        {f1:.4f}\")\n",
    "    if roc is not None:\n",
    "        print(f\"ROC AUC:         {roc:.4f}\")\n",
    "    print(f\"PR AUC:          {prauc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return {\n",
    "        \"loss\": val_loss,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc,\n",
    "        \"pr_auc\": prauc,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": report\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0cef1b43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Loss: 0.0384\n",
      "Accuracy:        0.8100\n",
      "Precision:       0.9733\n",
      "Recall:          0.6375\n",
      "F1 Score:        0.7704\n",
      "ROC AUC:         0.9500\n",
      "PR AUC:          0.9515\n",
      "\n",
      "Confusion Matrix:\n",
      "[[786  14]\n",
      " [290 510]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0     0.7305    0.9825    0.8380       800\n",
      "         1.0     0.9733    0.6375    0.7704       800\n",
      "\n",
      "    accuracy                         0.8100      1600\n",
      "   macro avg     0.8519    0.8100    0.8042      1600\n",
      "weighted avg     0.8519    0.8100    0.8042      1600\n",
      "\n"
     ]
    }
   ],
   "source": [
    "metrics = full_validate(model, val_loader, criterion, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e84ec30",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
