{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9f6e0724",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import random\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from collections import Counter\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f46dd210",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8000/8000 [00:01<00:00, 6964.44it/s]\n"
     ]
    }
   ],
   "source": [
    "# --- Setup\n",
    "SEED = 42\n",
    "torch.manual_seed(SEED)\n",
    "np.random.seed(SEED)\n",
    "OUT_DIR = \"/content/flight_optuna_best\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# --- Synthetic balanced dataset\n",
    "N_SEQ, TIMESTEPS, FEATURES = 8000, 60, 7\n",
    "def generate_sequence(is_incident, timesteps=60):\n",
    "    t = np.linspace(0,1,timesteps)\n",
    "    alt = 10000 + 200*np.sin(2*np.pi*t) + np.random.normal(0,8,timesteps)\n",
    "    spd = 250 + 8*np.cos(2*np.pi*t) + np.random.normal(0,3,timesteps)\n",
    "    pitch, roll, yaw = np.random.normal(0,0.8,timesteps), np.random.normal(0,0.8,timesteps), np.random.normal(0,0.5,timesteps)\n",
    "    eng, thrust = 200+np.random.normal(0,2,timesteps), 70+np.random.normal(0,2,timesteps)\n",
    "    seq = np.vstack([alt,spd,pitch,roll,yaw,eng,thrust]).T\n",
    "    weather, phase, maint = random.choice([\"clear\",\"rain\",\"storm\",\"fog\",\"turbulence\"]), random.choice([\"cruise\",\"takeoff\",\"landing\",\"climb\",\"descent\"]), np.random.randint(0,3)\n",
    "    if is_incident:\n",
    "        pat = random.choice([\"rapid_descent\",\"stall\",\"engine_fail\"])\n",
    "        if pat==\"rapid_descent\": seq[:,0]-=np.linspace(0,np.random.randint(1000,1500),timesteps)\n",
    "        if pat==\"stall\": seq[:,1]-=np.linspace(0,np.random.randint(60,120),timesteps); seq[:,2]+=np.linspace(0,np.random.randint(4,8),timesteps)\n",
    "        if pat==\"engine_fail\": seq[:,5]+=np.linspace(0,np.random.randint(100,150),timesteps); seq[:,6]-=np.linspace(0,np.random.randint(30,50),timesteps)\n",
    "        maint+=np.random.randint(1,2); weather=random.choice([\"storm\",\"turbulence\",\"fog\"])\n",
    "    return seq, weather, phase, maint, int(is_incident)\n",
    "\n",
    "seqs, metas, labels = [], [], []\n",
    "for i in tqdm(range(N_SEQ)):\n",
    "    seq,w,p,m,y = generate_sequence(i>=N_SEQ//2)\n",
    "    seqs.append(seq); metas.append((w,p,m)); labels.append(y)\n",
    "seqs=np.array(seqs); metas=pd.DataFrame(metas,columns=[\"weather\",\"phase\",\"maint\"]); labels=np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "56b70a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c12bb3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, dataloader, criterion, optimizer, device, accumulation_steps=4):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    for i, (sequences, metadata, labels) in enumerate(dataloader):\n",
    "        sequences = sequences.to(device)\n",
    "        metadata = metadata.to(device)\n",
    "        labels = labels.to(device).unsqueeze(1)\n",
    "        \n",
    "        # forward pass\n",
    "        outputs = model(sequences, metadata)\n",
    "        loss = criterion(outputs, labels)\n",
    "        \n",
    "        # normalize loss by accumulation steps\n",
    "        loss = loss / accumulation_steps\n",
    "        loss.backward()\n",
    "        \n",
    "        # update weights every accumulation_steps\n",
    "        if (i + 1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        \n",
    "        total_loss += loss.item() * accumulation_steps  # scale back the loss\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        predictions = (torch.sigmoid(outputs) > 0.5).float()\n",
    "        correct += (predictions == labels).sum().item()\n",
    "        total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1b68c8ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.inference_mode():\n",
    "        for sequences, metadata, labels in dataloader:\n",
    "            sequences = sequences.to(device)\n",
    "            metadata = metadata.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            \n",
    "            outputs = model(sequences, metadata)\n",
    "            loss = criterion(outputs, labels)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            probs = torch.sigmoid(outputs)\n",
    "            predictions = (probs > 0.5).float()\n",
    "            \n",
    "            correct += (predictions == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    return total_loss / len(dataloader), correct / total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "431ceccf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: 8000 samples, 60 timesteps, 7 features\n"
     ]
    }
   ],
   "source": [
    "N, T, F = seqs.shape\n",
    "print(f\"Data shape: {N} samples, {T} timesteps, {F} features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5d27a4af",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess metadata\n",
    "cat_cols = [\"weather\", \"phase\"]\n",
    "num_cols = [\"maint\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "252fdaca",
   "metadata": {},
   "outputs": [],
   "source": [
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols),\n",
    "        (\"num\", StandardScaler(), num_cols),\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc919b34",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_meta_processed = preprocessor.fit_transform(metas)\n",
    "\n",
    "if hasattr(X_meta_processed, 'toarray'):\n",
    "    X_meta_processed = X_meta_processed.toarray()\n",
    "    \n",
    "num_meta_features = X_meta_processed.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "99f14f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training: 6400 samples\n",
      "Validation: 1600 samples\n"
     ]
    }
   ],
   "source": [
    "# Train/validation split\n",
    "X_seq_train, X_seq_val, X_meta_train, X_meta_val, y_train, y_val = train_test_split(\n",
    "    seqs, X_meta_processed, labels, \n",
    "    test_size=0.2, \n",
    "    stratify=labels, \n",
    "    random_state=SEED\n",
    ")\n",
    "\n",
    "print(f\"Training: {len(X_seq_train)} samples\")\n",
    "print(f\"Validation: {len(X_seq_val)} samples\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "359306be",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformer import AviationTransformer\n",
    "\n",
    "# Create model\n",
    "model = AviationTransformer(\n",
    "    num_features=F,\n",
    "    num_meta_features=num_meta_features,\n",
    "    d_model=128,\n",
    "    nhead=8,\n",
    "    num_layers=4,\n",
    "    dim_feedforward=512,\n",
    "    dropout=0.15,\n",
    "    max_seq_len=T\n",
    ").to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1aa00fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_optimal_batch_size(model, device, max_batch=512):\n",
    "    \"\"\"\n",
    "    Binary search for the largest batch size that fits in memory,\n",
    "    using the model's actual metadata dimension.\n",
    "    \"\"\"\n",
    "    # Extract metadata dimension from model\n",
    "    # meta_fc is: Linear(in_features=num_meta_features, out_features=64)\n",
    "    num_meta = model.meta_fc[0].in_features\n",
    "\n",
    "    batch_size = 32\n",
    "    while batch_size <= max_batch:\n",
    "        try:\n",
    "            # Create dummy inputs with the correct shapes\n",
    "            dummy_seq  = torch.randn(batch_size, 60, 7, device=device)\n",
    "            dummy_meta = torch.randn(batch_size, num_meta, device=device)\n",
    "\n",
    "            outputs = model(dummy_seq, dummy_meta)\n",
    "            loss = outputs.sum()\n",
    "            loss.backward()\n",
    "            return batch_size  # success\n",
    "\n",
    "        except RuntimeError as e:\n",
    "            if 'out of memory' in str(e):\n",
    "                # Too large: try half\n",
    "                torch.cuda.empty_cache()\n",
    "                batch_size //= 2\n",
    "                if batch_size == 0:\n",
    "                    raise RuntimeError(\"Not enough GPU memory even for batch_size=1\")\n",
    "            else:\n",
    "                # Propagate other errors\n",
    "                raise e\n",
    "\n",
    "    return batch_size\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "3df78cba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimal batch size: 32\n"
     ]
    }
   ],
   "source": [
    "optimal_batch = find_optimal_batch_size(model, device)\n",
    "print(f\"Optimal batch size: {optimal_batch}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "dee2c747",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model compiled with torch.compile()\n"
     ]
    }
   ],
   "source": [
    "# Compile the model with torch 2.0+\n",
    "if hasattr(torch, 'compile'):\n",
    "    model = torch.compile(\n",
    "        model,\n",
    "        mode='reduce-overhead',  # Options: 'default', 'reduce-overhead', 'max-autotune'\n",
    "        fullgraph=True,          # Try to compile entire model as one graph\n",
    "        dynamic=False            # Assumes fixed input shapes (faster)\n",
    "    )\n",
    "    print(\"Model compiled with torch.compile()\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c65dc7d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataset import AviationDataset\n",
    "\n",
    "# Create datasets and dataloaders\n",
    "train_dataset = AviationDataset(X_seq_train, X_meta_train, y_train)\n",
    "val_dataset = AviationDataset(X_seq_val, X_meta_val, y_val)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=optimal_batch, shuffle=True, \n",
    "                          num_workers=4, pin_memory=True, prefetch_factor=2, \n",
    "                          persistent_workers=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=optimal_batch, shuffle=False, \n",
    "                        num_workers=2, pin_memory=True, prefetch_factor=2, \n",
    "                        persistent_workers=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "cc74bd1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Model Architecture:\n",
      "OptimizedModule(\n",
      "  (_orig_mod): AviationTransformer(\n",
      "    (input_projection): Linear(in_features=7, out_features=128, bias=True)\n",
      "    (pos_encoder): PositionalEncoding(\n",
      "      (dropout): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "    (transformer_encoder): TransformerEncoder(\n",
      "      (layers): ModuleList(\n",
      "        (0-3): 4 x TransformerEncoderLayer(\n",
      "          (self_attn): MultiheadAttention(\n",
      "            (out_proj): NonDynamicallyQuantizableLinear(in_features=128, out_features=128, bias=True)\n",
      "          )\n",
      "          (linear1): Linear(in_features=128, out_features=512, bias=True)\n",
      "          (dropout): Dropout(p=0.15, inplace=False)\n",
      "          (linear2): Linear(in_features=512, out_features=128, bias=True)\n",
      "          (norm1): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (norm2): LayerNorm((128,), eps=1e-05, elementwise_affine=True)\n",
      "          (dropout1): Dropout(p=0.15, inplace=False)\n",
      "          (dropout2): Dropout(p=0.15, inplace=False)\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (meta_fc): Sequential(\n",
      "      (0): Linear(in_features=11, out_features=64, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.15, inplace=False)\n",
      "    )\n",
      "    (classifier): Sequential(\n",
      "      (0): Linear(in_features=192, out_features=256, bias=True)\n",
      "      (1): ReLU()\n",
      "      (2): Dropout(p=0.15, inplace=False)\n",
      "      (3): Linear(in_features=256, out_features=128, bias=True)\n",
      "      (4): ReLU()\n",
      "      (5): Dropout(p=0.15, inplace=False)\n",
      "      (6): Linear(in_features=128, out_features=1, bias=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "\n",
      "Total parameters: 877,313\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nModel Architecture:\")\n",
    "print(model)\n",
    "print(f\"\\nTotal parameters: {sum(p.numel() for p in model.parameters()):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a4a3f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optimizer and scheduler\n",
    "optimizer = torch.optim.AdamW(model.parameters(),\n",
    "                              lr=0.001,\n",
    "                              weight_decay=1e-4,\n",
    "                              fused=True)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "    optimizer, mode='min', factor=0.5, patience=8\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33c43023",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate class weights\n",
    "class_counts = Counter(y_train)\n",
    "pos_weight = torch.tensor([class_counts[0] / class_counts[1]]).to(device)\n",
    "criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "866a3ee4",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.makedirs('models', exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "bb65e5e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:668: UserWarning: 'pin_memory' argument is set as true but no accelerator is found, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "cudagraph partition due to non gpu ops\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 78, in forward\n",
      "    meta_features = self.meta_fc(meta_input)  # (batch, 64)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 66, in forward\n",
      "    x = self.input_projection(seq_input)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 934, in forward\n",
      "    x\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 937, in forward\n",
      "    x = self.norm2(x + self._ff_block(x))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 962, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 937, in forward\n",
      "    x = self.norm2(x + self._ff_block(x))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 962, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 934, in forward\n",
      "    x\n",
      "\n",
      "cudagraph partition due to non gpu ops\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 933, in forward\n",
      "    x = self.norm1(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 229, in forward\n",
      "    return F.layer_norm(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 937, in forward\n",
      "    x = self.norm2(x + self._ff_block(x))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 962, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 937, in forward\n",
      "    x = self.norm2(x + self._ff_block(x))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 962, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 934, in forward\n",
      "    x\n",
      "\n",
      "cudagraph partition due to non gpu ops\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 933, in forward\n",
      "    x = self.norm1(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 229, in forward\n",
      "    return F.layer_norm(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 937, in forward\n",
      "    x = self.norm2(x + self._ff_block(x))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 962, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 937, in forward\n",
      "    x = self.norm2(x + self._ff_block(x))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 962, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 934, in forward\n",
      "    x\n",
      "\n",
      "cudagraph partition due to non gpu ops\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 933, in forward\n",
      "    x = self.norm1(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 229, in forward\n",
      "    return F.layer_norm(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 937, in forward\n",
      "    x = self.norm2(x + self._ff_block(x))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 962, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 937, in forward\n",
      "    x = self.norm2(x + self._ff_block(x))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 962, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 81, in forward\n",
      "    combined = torch.cat([seq_features, meta_features], dim=1)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 84, in forward\n",
      "    logits = self.classifier(combined)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 84, in forward\n",
      "    logits = self.classifier(combined)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 937, in forward\n",
      "    x = self.norm2(x + self._ff_block(x))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 962, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 933, in forward\n",
      "    x = self.norm1(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 229, in forward\n",
      "    return F.layer_norm(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 937, in forward\n",
      "    x = self.norm2(x + self._ff_block(x))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 962, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 933, in forward\n",
      "    x = self.norm1(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 229, in forward\n",
      "    return F.layer_norm(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 937, in forward\n",
      "    x = self.norm2(x + self._ff_block(x))\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 962, in _ff_block\n",
      "    x = self.linear2(self.dropout(self.activation(self.linear1(x))))\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 935, in forward\n",
      "    + self._sa_block(x, src_mask, src_key_padding_mask, is_causal=is_causal)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 949, in _sa_block\n",
      "    x = self.self_attn(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\activation.py\", line 1488, in forward\n",
      "    attn_output, attn_output_weights = F.multi_head_attention_forward(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 72, in forward\n",
      "    x = self.transformer_encoder(x)  # (batch, seq_len, d_model)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 524, in forward\n",
      "    output = mod(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\transformer.py\", line 933, in forward\n",
      "    x = self.norm1(\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\normalization.py\", line 229, in forward\n",
      "    return F.layer_norm(\n",
      "\n",
      "cudagraph partition due to non gpu ops. Found from : \n",
      "   File \"c:\\Users\\Marc\\Desktop\\Programming\\LABS\\FDL-REPORT\\transformer.py\", line 84, in forward\n",
      "    logits = self.classifier(combined)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\container.py\", line 250, in forward\n",
      "    input = module(input)\n",
      "  File \"c:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\linear.py\", line 134, in forward\n",
      "    return F.linear(input, self.weight, self.bias)\n",
      "\n"
     ]
    },
    {
     "ename": "InductorError",
     "evalue": "RuntimeError: Compiler: cl is not found.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mInductorError\u001b[0m                             Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[22], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mStarting training...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_epochs):\n\u001b[1;32m---> 14\u001b[0m     train_loss, train_acc \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m     val_loss, val_acc \u001b[38;5;241m=\u001b[39m validate(\n\u001b[0;32m     16\u001b[0m         model, val_loader, criterion, device\n\u001b[0;32m     17\u001b[0m     )\n\u001b[0;32m     19\u001b[0m     train_losses\u001b[38;5;241m.\u001b[39mappend(train_loss)\n",
      "Cell \u001b[1;32mIn[4], line 13\u001b[0m, in \u001b[0;36mtrain_epoch\u001b[1;34m(model, dataloader, criterion, optimizer, device, accumulation_steps)\u001b[0m\n\u001b[0;32m     10\u001b[0m labels \u001b[38;5;241m=\u001b[39m labels\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# forward pass\u001b[39;00m\n\u001b[1;32m---> 13\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43msequences\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# normalize loss by accumulation steps\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:414\u001b[0m, in \u001b[0;36mOptimizedModule.__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mmodules\u001b[38;5;241m.\u001b[39mmodule\u001b[38;5;241m.\u001b[39m_has_any_global_hook():\n\u001b[0;32m    405\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUsing `torch.compile(module)` when there are global hooks on \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    407\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmodules (e.g., from `register_module_forward_hook`); this will\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    412\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[0;32m    413\u001b[0m     )\n\u001b[1;32m--> 414\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__call__\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1773\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1774\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1775\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1781\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1782\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1783\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1784\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1785\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1786\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1788\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1789\u001b[0m called_always_called_hooks \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mset\u001b[39m()\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_dynamo\\eval_frame.py:845\u001b[0m, in \u001b[0;36m_TorchDynamoContext.__call__.<locals>.compile_wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    841\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01me\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m__cause__\u001b[39;00m  \u001b[38;5;66;03m# User compiler error\u001b[39;00m\n\u001b[0;32m    842\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m ShortenTraceback \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    843\u001b[0m     \u001b[38;5;66;03m# Failures in the backend likely don't have useful\u001b[39;00m\n\u001b[0;32m    844\u001b[0m     \u001b[38;5;66;03m# data in the TorchDynamo frames, so we strip them out.\u001b[39;00m\n\u001b[1;32m--> 845\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mremove_dynamo_frames() \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m  \u001b[38;5;66;03m# see TORCHDYNAMO_VERBOSE=1\u001b[39;00m\n\u001b[0;32m    846\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    847\u001b[0m     \u001b[38;5;66;03m# Restore the dynamic layer stack depth if necessary.\u001b[39;00m\n\u001b[0;32m    848\u001b[0m     set_eval_frame(\u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:990\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[1;34m(gm, example_inputs, **graph_kwargs)\u001b[0m\n\u001b[0;32m    988\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n\u001b[0;32m    989\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m--> 990\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m InductorError(e, currentframe())\u001b[38;5;241m.\u001b[39mwith_traceback(\n\u001b[0;32m    991\u001b[0m         e\u001b[38;5;241m.\u001b[39m__traceback__\n\u001b[0;32m    992\u001b[0m     ) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    993\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    994\u001b[0m     TritonBundler\u001b[38;5;241m.\u001b[39mend_compile()\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:974\u001b[0m, in \u001b[0;36m_compile_fx_inner\u001b[1;34m(gm, example_inputs, **graph_kwargs)\u001b[0m\n\u001b[0;32m    972\u001b[0m TritonBundler\u001b[38;5;241m.\u001b[39mbegin_compile()\n\u001b[0;32m    973\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 974\u001b[0m     mb_compiled_graph \u001b[38;5;241m=\u001b[39m fx_codegen_and_compile(\n\u001b[0;32m    975\u001b[0m         gm, example_inputs, inputs_to_check, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mgraph_kwargs\n\u001b[0;32m    976\u001b[0m     )\n\u001b[0;32m    977\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m mb_compiled_graph \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    978\u001b[0m     mb_compiled_graph\u001b[38;5;241m.\u001b[39m_time_taken_ns \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime_ns() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:1695\u001b[0m, in \u001b[0;36mfx_codegen_and_compile\u001b[1;34m(gm, example_inputs, inputs_to_check, **graph_kwargs)\u001b[0m\n\u001b[0;32m   1691\u001b[0m     fast_scheme \u001b[38;5;241m=\u001b[39m _InProcessFxCompile()\n\u001b[0;32m   1693\u001b[0m     scheme \u001b[38;5;241m=\u001b[39m _ProgressiveFxCompile(fast_scheme, scheme, progression_configs)\n\u001b[1;32m-> 1695\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mscheme\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_and_compile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgm\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexample_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs_to_check\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgraph_kwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\compile_fx.py:1505\u001b[0m, in \u001b[0;36m_InProcessFxCompile.codegen_and_compile\u001b[1;34m(self, gm, example_inputs, inputs_to_check, graph_kwargs)\u001b[0m\n\u001b[0;32m   1487\u001b[0m         compiled_fn \u001b[38;5;241m=\u001b[39m AotCodeCompiler\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[0;32m   1488\u001b[0m             graph,\n\u001b[0;32m   1489\u001b[0m             wrapper_code\u001b[38;5;241m.\u001b[39mvalue,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1502\u001b[0m             ],\n\u001b[0;32m   1503\u001b[0m         )\n\u001b[0;32m   1504\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1505\u001b[0m     compiled_module \u001b[38;5;241m=\u001b[39m \u001b[43mgraph\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1506\u001b[0m     compiled_fn \u001b[38;5;241m=\u001b[39m compiled_module\u001b[38;5;241m.\u001b[39mcall\n\u001b[0;32m   1507\u001b[0m     compiled_fn_runner \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(\n\u001b[0;32m   1508\u001b[0m         compiled_module, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrunner\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1509\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\graph.py:2319\u001b[0m, in \u001b[0;36mGraphLowering.compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2312\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcompile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledModule:\n\u001b[0;32m   2313\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\n\u001b[0;32m   2314\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mGraphLowering.compile_to_module\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2315\u001b[0m         phase_name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcode_gen\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2316\u001b[0m         log_pt2_compile_event\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m   2317\u001b[0m         dynamo_compile_column_us\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minductor_code_gen_cumulative_compile_time_us\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2318\u001b[0m     ):\n\u001b[1;32m-> 2319\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_compile_to_module\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\graph.py:2325\u001b[0m, in \u001b[0;36mGraphLowering._compile_to_module\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2321\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_compile_to_module\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m CompiledModule:\n\u001b[0;32m   2322\u001b[0m     \u001b[38;5;66;03m# If we're here, we don't have to worry about the kernel code, which is only\u001b[39;00m\n\u001b[0;32m   2323\u001b[0m     \u001b[38;5;66;03m# returned separately in AOTInductor mode.\u001b[39;00m\n\u001b[0;32m   2324\u001b[0m     wrapper_code, _ \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m-> 2325\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcodegen_with_cpp_wrapper() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcpp_wrapper \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2326\u001b[0m     )\n\u001b[0;32m   2328\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(wrapper_code, ValueWithLineMap):\n\u001b[0;32m   2329\u001b[0m         mod \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compile_to_module_lines(wrapper_code)\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\graph.py:2264\u001b[0m, in \u001b[0;36mGraphLowering.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   2261\u001b[0m V\u001b[38;5;241m.\u001b[39mdebug\u001b[38;5;241m.\u001b[39mdraw_orig_fx_graph(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39morig_gm, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscheduler\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[0;32m   2263\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_code\u001b[38;5;241m.\u001b[39mpush_codegened_graph(\u001b[38;5;28mself\u001b[39m)\n\u001b[1;32m-> 2264\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mscheduler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2266\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\n\u001b[0;32m   2267\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFinished codegen for all nodes. The list of kernel names available: \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m   2268\u001b[0m     V\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mall_codegen_kernel_names,\n\u001b[0;32m   2269\u001b[0m )\n\u001b[0;32m   2271\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwrapper_code\u001b[38;5;241m.\u001b[39mgenerate(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mis_inference)\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\scheduler.py:5205\u001b[0m, in \u001b[0;36mScheduler.codegen\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5202\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mcodegen\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m   5203\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m dynamo_timed(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScheduler.codegen\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m   5204\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\n\u001b[1;32m-> 5205\u001b[0m             \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_codegen_partitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5206\u001b[0m             \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_inductor\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mgraph_partition\n\u001b[0;32m   5207\u001b[0m             \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_codegen(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnodes)\n\u001b[0;32m   5208\u001b[0m         )\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\scheduler.py:5345\u001b[0m, in \u001b[0;36mScheduler._codegen_partitions\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   5340\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(partition) \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m, (\n\u001b[0;32m   5341\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEach partition must have at least one node but found \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlen\u001b[39m(partition)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   5342\u001b[0m )\n\u001b[0;32m   5344\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m signature\u001b[38;5;241m.\u001b[39mskip_cudagraph:\n\u001b[1;32m-> 5345\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_codegen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpartition\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5346\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5347\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_codegen_partition_wrapper(partition, signature)\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\scheduler.py:5443\u001b[0m, in \u001b[0;36mScheduler._codegen\u001b[1;34m(self, nodes)\u001b[0m\n\u001b[0;32m   5441\u001b[0m     backend\u001b[38;5;241m.\u001b[39mcodegen_combo_kernel(node)\n\u001b[0;32m   5442\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, (FusedSchedulerNode, SchedulerNode)):\n\u001b[1;32m-> 5443\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcodegen_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5444\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   5445\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, NopKernelSchedulerNode)\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:5282\u001b[0m, in \u001b[0;36mCppScheduling.codegen_node\u001b[1;34m(self, node)\u001b[0m\n\u001b[0;32m   5280\u001b[0m nodes: \u001b[38;5;28mlist\u001b[39m[SchedulerNode] \u001b[38;5;241m=\u001b[39m node\u001b[38;5;241m.\u001b[39mget_nodes()  \u001b[38;5;66;03m# type: ignore[assignment]\u001b[39;00m\n\u001b[0;32m   5281\u001b[0m nodes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtry_loop_split(nodes)\n\u001b[1;32m-> 5282\u001b[0m cpp_kernel_proxy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_proxy_cls\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkernel_group\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5283\u001b[0m cpp_kernel_proxy\u001b[38;5;241m.\u001b[39mcodegen_nodes(nodes)\n\u001b[0;32m   5284\u001b[0m kernel_group\u001b[38;5;241m.\u001b[39mfinalize_kernel(cpp_kernel_proxy, nodes)\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\codegen\\cpp.py:4008\u001b[0m, in \u001b[0;36mCppKernelProxy.__init__\u001b[1;34m(self, kernel_group)\u001b[0m\n\u001b[0;32m   4006\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mloop_nest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   4007\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcall_ranges \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 4008\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpicked_vec_isa: cpu_vec_isa\u001b[38;5;241m.\u001b[39mVecISA \u001b[38;5;241m=\u001b[39m \u001b[43mcpu_vec_isa\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpick_vec_isa\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   4009\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mkernels: \u001b[38;5;28mlist\u001b[39m[CppKernel] \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:497\u001b[0m, in \u001b[0;36mpick_vec_isa\u001b[1;34m()\u001b[0m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_fbcode() \u001b[38;5;129;01mand\u001b[39;00m (platform\u001b[38;5;241m.\u001b[39mmachine() \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mx86_64\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAMD64\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m    495\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m VecAVX2()\n\u001b[1;32m--> 497\u001b[0m _valid_vec_isa_list: \u001b[38;5;28mlist\u001b[39m[VecISA] \u001b[38;5;241m=\u001b[39m \u001b[43mvalid_vec_isa_list\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _valid_vec_isa_list:\n\u001b[0;32m    499\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m invalid_vec_isa\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:484\u001b[0m, in \u001b[0;36mvalid_vec_isa_list\u001b[1;34m()\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     _cpu_supported_x86_isa \u001b[38;5;241m=\u001b[39m x86_isa_checker()\n\u001b[1;32m--> 484\u001b[0m     \u001b[43misa_list\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mextend\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    485\u001b[0m \u001b[43m        \u001b[49m\u001b[43misa\u001b[49m\n\u001b[0;32m    486\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43msupported_vec_isa_list\u001b[49m\n\u001b[0;32m    487\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mall\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_cpu_supported_x86_isa\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mflag\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43misa\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mand\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43misa\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:484\u001b[0m, in \u001b[0;36m<genexpr>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m    481\u001b[0m \u001b[38;5;124;03m    arch value is x86_64 on Linux, and the value is AMD64 on Windows.\u001b[39;00m\n\u001b[0;32m    482\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m    483\u001b[0m     _cpu_supported_x86_isa \u001b[38;5;241m=\u001b[39m x86_isa_checker()\n\u001b[1;32m--> 484\u001b[0m     isa_list\u001b[38;5;241m.\u001b[39mextend(\n\u001b[0;32m    485\u001b[0m         isa\n\u001b[0;32m    486\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m isa \u001b[38;5;129;01min\u001b[39;00m supported_vec_isa_list\n\u001b[0;32m    487\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(flag \u001b[38;5;129;01min\u001b[39;00m _cpu_supported_x86_isa \u001b[38;5;28;01mfor\u001b[39;00m flag \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mstr\u001b[39m(isa)\u001b[38;5;241m.\u001b[39msplit()) \u001b[38;5;129;01mand\u001b[39;00m isa\n\u001b[0;32m    488\u001b[0m     )\n\u001b[0;32m    490\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m isa_list\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:143\u001b[0m, in \u001b[0;36mVecISA.__bool__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__bool__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n\u001b[1;32m--> 143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__bool__impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcpp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvec_isa_ok\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:153\u001b[0m, in \u001b[0;36mVecISA.__bool__impl\u001b[1;34m(self, vec_isa_ok)\u001b[0m\n\u001b[0;32m    150\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m config\u001b[38;5;241m.\u001b[39mis_fbcode():\n\u001b[0;32m    151\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 153\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcheck_build\u001b[49m\u001b[43m(\u001b[49m\u001b[43mVecISA\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_avx_code\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:103\u001b[0m, in \u001b[0;36mVecISA.check_build\u001b[1;34m(self, code)\u001b[0m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcodecache\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_lock_dir, LOCK_TIMEOUT, write\n\u001b[0;32m     94\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[0;32m     95\u001b[0m     CppBuilder,\n\u001b[0;32m     96\u001b[0m     CppTorchOptions,\n\u001b[0;32m     97\u001b[0m     normalize_path_separator,\n\u001b[0;32m     98\u001b[0m )\n\u001b[0;32m    100\u001b[0m key, input_path \u001b[38;5;241m=\u001b[39m write(\n\u001b[0;32m    101\u001b[0m     code,\n\u001b[0;32m    102\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcpp\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m--> 103\u001b[0m     extra\u001b[38;5;241m=\u001b[39m\u001b[43m_get_isa_dry_compile_fingerprint\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_arch_flags\u001b[49m\u001b[43m)\u001b[49m,\n\u001b[0;32m    104\u001b[0m )\n\u001b[0;32m    105\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_filelock\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FileLock\n\u001b[0;32m    107\u001b[0m lock_dir \u001b[38;5;241m=\u001b[39m get_lock_dir()\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\cpu_vec_isa.py:29\u001b[0m, in \u001b[0;36m_get_isa_dry_compile_fingerprint\u001b[1;34m(isa_flags)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_get_isa_dry_compile_fingerprint\u001b[39m(isa_flags: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mstr\u001b[39m:\n\u001b[0;32m     21\u001b[0m     \u001b[38;5;66;03m# ISA dry compile will cost about 1 sec time each startup time.\u001b[39;00m\n\u001b[0;32m     22\u001b[0m     \u001b[38;5;66;03m# Please check the issue: https://github.com/pytorch/pytorch/issues/100378\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     25\u001b[0m     \u001b[38;5;66;03m# and generated them to output binary hash path.\u001b[39;00m\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;66;03m# It would optimize and skip compile existing binary.\u001b[39;00m\n\u001b[0;32m     27\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_inductor\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcpp_builder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_compiler_version_info, get_cpp_compiler\n\u001b[1;32m---> 29\u001b[0m     compiler_info \u001b[38;5;241m=\u001b[39m get_compiler_version_info(\u001b[43mget_cpp_compiler\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     30\u001b[0m     torch_version \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39m__version__\n\u001b[0;32m     31\u001b[0m     fingerprint \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler_info\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00misa_flags\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtorch_version\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\cpp_builder.py:338\u001b[0m, in \u001b[0;36mget_cpp_compiler\u001b[1;34m()\u001b[0m\n\u001b[0;32m    336\u001b[0m     compiler \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39menviron\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCXX\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    337\u001b[0m     compiler \u001b[38;5;241m=\u001b[39m normalize_path_separator(compiler)\n\u001b[1;32m--> 338\u001b[0m     \u001b[43mcheck_compiler_exist_windows\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcompiler\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    339\u001b[0m     check_msvc_cl_language_id(compiler)\n\u001b[0;32m    340\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\Marc\\anaconda3\\envs\\casenv\\lib\\site-packages\\torch\\_inductor\\cpp_builder.py:139\u001b[0m, in \u001b[0;36mcheck_compiler_exist_windows\u001b[1;34m(compiler)\u001b[0m\n\u001b[0;32m    137\u001b[0m     subprocess\u001b[38;5;241m.\u001b[39mcheck_output([compiler, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/help\u001b[39m\u001b[38;5;124m\"\u001b[39m], stderr\u001b[38;5;241m=\u001b[39msubprocess\u001b[38;5;241m.\u001b[39mSTDOUT)\n\u001b[0;32m    138\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m exc:\n\u001b[1;32m--> 139\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCompiler: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcompiler\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m is not found.\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mexc\u001b[39;00m\n\u001b[0;32m    140\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m subprocess\u001b[38;5;241m.\u001b[39mSubprocessError:\n\u001b[0;32m    141\u001b[0m     \u001b[38;5;66;03m# Expected that some compiler(clang, clang++) is exist, but they not support `/help` args.\u001b[39;00m\n\u001b[0;32m    142\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n",
      "\u001b[1;31mInductorError\u001b[0m: RuntimeError: Compiler: cl is not found.\n\nSet TORCHDYNAMO_VERBOSE=1 for the internal stack trace (please do this especially if you're reporting a bug to PyTorch). For even more developer context, set TORCH_LOGS=\"+dynamo\"\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_epochs = 100\n",
    "best_val_loss = float('inf')\n",
    "patience = 15\n",
    "patience_counter = 0\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accs = []\n",
    "val_accs = []\n",
    "\n",
    "print(\"\\nStarting training...\")\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, device)\n",
    "    val_loss, val_acc = validate(\n",
    "        model, val_loader, criterion, device\n",
    "    )\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    \n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}\")\n",
    "    print(f\"  Train Loss: {train_loss:.4f}, Train Acc: {train_acc:.4f}\")\n",
    "    print(f\"  Val Loss: {val_loss:.4f}, Val Acc: {val_acc:.4f}\")\n",
    "    \n",
    "    # Early stopping\n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        patience_counter = 0\n",
    "        torch.save(model.state_dict(), 'models/best_aviation_transformer.pt')\n",
    "        print(\"  → Model saved!\")\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "        if patience_counter >= patience:\n",
    "            print(f\"\\nEarly stopping after {epoch+1} epochs\")\n",
    "            break\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db770275",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load best model\n",
    "model.load_state_dict(torch.load('best_aviation_transformer.pt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea6a348b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import (\n",
    "    classification_report,\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    "    f1_score,\n",
    "    roc_auc_score,\n",
    "    average_precision_score\n",
    ")\n",
    "\n",
    "def full_validate(model, dataloader, criterion, device):\n",
    "    # Run your existing validate to get loss and accuracy\n",
    "    val_loss, val_acc = validate(model, dataloader, criterion, device)\n",
    "\n",
    "    # Now collect all labels, predictions, and probabilities\n",
    "    model.eval()\n",
    "    all_labels, all_preds, all_probs = [], [], []\n",
    "    with torch.inference_mode():\n",
    "        for seqs, metas, labels in dataloader:\n",
    "            seqs, metas = seqs.to(device), metas.to(device)\n",
    "            labels = labels.to(device).unsqueeze(1)\n",
    "            outputs = model(seqs, metas)\n",
    "            probs = torch.sigmoid(outputs)\n",
    "            preds = (probs > 0.5).float()\n",
    "\n",
    "            all_labels.extend(labels.cpu().numpy().flatten())\n",
    "            all_preds.extend(preds.cpu().numpy().flatten())\n",
    "            all_probs.extend(probs.cpu().numpy().flatten())\n",
    "\n",
    "    # Convert to numpy arrays\n",
    "    y_true = np.array(all_labels)\n",
    "    y_pred = np.array(all_preds)\n",
    "    y_prob = np.array(all_probs)\n",
    "\n",
    "    # Compute standard metrics\n",
    "    acc   = accuracy_score(y_true, y_pred)\n",
    "    prec  = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec   = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1    = f1_score(y_true, y_pred, zero_division=0)\n",
    "    roc   = roc_auc_score(y_true, y_prob) if len(np.unique(y_true))>1 else None\n",
    "    prauc = average_precision_score(y_true, y_prob)\n",
    "\n",
    "    # Confusion matrix and classification report\n",
    "    cm     = confusion_matrix(y_true, y_pred)\n",
    "    report = classification_report(y_true, y_pred, digits=4)\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Validation Loss: {val_loss:.4f}\")\n",
    "    print(f\"Accuracy:        {acc:.4f}\")\n",
    "    print(f\"Precision:       {prec:.4f}\")\n",
    "    print(f\"Recall:          {rec:.4f}\")\n",
    "    print(f\"F1 Score:        {f1:.4f}\")\n",
    "    if roc is not None:\n",
    "        print(f\"ROC AUC:         {roc:.4f}\")\n",
    "    print(f\"PR AUC:          {prauc:.4f}\")\n",
    "    print(\"\\nConfusion Matrix:\")\n",
    "    print(cm)\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(report)\n",
    "\n",
    "    return {\n",
    "        \"loss\": val_loss,\n",
    "        \"accuracy\": acc,\n",
    "        \"precision\": prec,\n",
    "        \"recall\": rec,\n",
    "        \"f1\": f1,\n",
    "        \"roc_auc\": roc,\n",
    "        \"pr_auc\": prauc,\n",
    "        \"confusion_matrix\": cm,\n",
    "        \"classification_report\": report\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cef1b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics = full_validate(model, val_loader, criterion, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "casenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
